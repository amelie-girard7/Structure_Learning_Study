{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Statistical Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Objectives\n",
    "\n",
    "Statistics is concern with the entire process of planning, collecting and analyzing data to draw scientifically defensible conclusions. \n",
    "\n",
    "A statistical model is a mathematical structure used to imitate and approximate the data generation process. It typically describes relationships among variables while accounting for unceratinty and variability in the data. \n",
    "\n",
    "**Objectives:**\n",
    "- Quantify uncertainty\n",
    "- Inference\n",
    "- Measuring the evidence in the data in support or against a hypothesis\n",
    "- Prediction\n",
    "\n",
    "How does Machine Learning relate to statistical modelling? Broadly, machine learning uses algorithms, some of which are based on or can be formulated as statistical models, to produce state of the art prediction. Machine learning speacializes in prediction. Often, excellent predictions come at the cost of building complicated models that are difficult to interpret. Objective four is almost always important, but does not necessarily all of the questions an investigator might have about their data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Statistical Modelling Process\n",
    "\n",
    "Building statistical models is a process and each step should be taken carefully.\n",
    "\n",
    "The statistical modelling process is as follows:\n",
    "- **Understand the problem and context**\n",
    "- **Plan and properly collect relevant data**\n",
    "- **Explore your data.** The practice of snooping around or mining your data, looking for interesting hypothesis to test can potentially invalidate your statistical modelling results. This is called data dredging. If you want to mine your data and test your findings it is usually best to randomly split your data into two parts. With one part, you can look for interesting things to test and fit different potential models. With the other, you can fit the model you chose during the first step to validate or see if the results can be replicated on other data.\n",
    "- **Postulate a model.** Generally, it is desireable to find a model where the parameters we estimate can be interpreted in the context of the problem. You will also need to balance between model model complexity and model generalizability.\n",
    "- **Fit the model.** In this step, we need to estimate the parameters of the model using the data. We will take a Bayesian approach to this step.\n",
    "- **Evaluate the model.** In this step, we will evaluate the model to see if the model accurately imitates the data generation process. \n",
    "- **Iterate**\n",
    "- **Use the Model.** We can use this model to answer questions about the data and arrive at conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
