{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Bayesian Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1. Model Specification\n",
    "\n",
    "Going back to our example\n",
    "\n",
    "We usually start with the model of the data directly or the likelihood:  \n",
    "\n",
    "$y_i~|~\\mu, \\sigma^2 \\sim ^{iid} N(\\mu, \\sigma^2) ~~~~ i = 1, ..., n$\n",
    "\n",
    "The next level that we need is the prior distribution from $\\mu$ and $\\sigma^2$. For now, we will consider they are independent priors, that is:\n",
    "\n",
    "$Pr( \\mu, \\sigma^2) = Pr( \\mu) Pr(\\sigma^2)$\n",
    "\n",
    "The conjugate prior for $\\mu$, if we know the value of $\\sigma^2$, is a normal distribution. And the conjugate prior for $\\sigma^2$ when $\\mu$ is known is an inverse gamma distribution.\n",
    "\n",
    "$\\mu \\sim N( \\mu_o, \\sigma_o^2)$ \n",
    "\n",
    "$\\sigma^2 \\sim IG(\\alpha_o, \\beta_o)$\n",
    "\n",
    "We will need to estimate those parameters, but this is the general Bayesian model for our problem. We can also represent this problem as a graphical model:\n",
    "\n",
    "<img src='./imgs/bayesian_model.png' width=200px />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2. Posterior Derivation\n",
    "\n",
    "$y_i~|~\\mu, \\sigma^2 \\sim ^{iid} N(\\mu, \\sigma^2) ~~~~ i = 1, ..., n$\n",
    "\n",
    "Instead of doing intdependet priors, we can do:\n",
    "\n",
    "$\\mu | \\sigma^2 \\sim N( \\mu_o, \\frac{\\sigma_o^2}{\\mu_0})$\n",
    "\n",
    "We will need to complete the model with the prior:\n",
    "\n",
    "$\\sigma_2 \\sim IG(v_0, \\beta_o)$\n",
    "\n",
    "Once we have the model specification, we can derive the posterior distribution. We can start by calculating the joint distribution of our model\n",
    "\n",
    "$Pr(y_1, ..., y_n, \\mu, \\sigma^2) = Pr(y_1, ..., y_n | \\mu, \\sigma^2) Pr(\\mu | \\sigma^2) Pr( \\sigma^2 )$\n",
    "\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~= \\prod_{i=1}^{n} [ N(y_i | \\mu, \\sigma^2)] N(\\mu | \\mu_0, \\frac{\\sigma^2}{w_0}) IG(\\sigma^2 | v_0, \\beta_0)$\n",
    "\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\approx Pr(\\mu, \\sigma^2 | y_1, ..., y_n)$\n",
    "\n",
    "The only thing missing in this expression is just some constant number that causes the expression to integrate to 1. If we can recognize this expression as being proportional to a common distribution, then  our work is done and we know what our posterior looks like. However, if we do not use conjugate priors or if the models are more complicated, then the posterior distribution will not have a standard form that we can recognize. In this case, we will need to use numerical methods to calculate the posterior distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3. Bayesian Modelling: Non-Conjugate Models\n",
    "\n",
    "In the previous section, we saw how to derive the posterior distribution for a conjugate model. However, in practice, we will often have non-conjugate models. In this section, we will see how to derive the posterior distribution for a non-conjugate model.\n",
    "\n",
    "**EXAMPLE 1**\n",
    "\n",
    "Consider the following model:\n",
    "\n",
    "For an unknown mean and a known variance, we have the following model:\n",
    "\n",
    "$y_i~|~\\mu~\\sim^{iid} N(\\mu, 1) ~~~~ i = 1, ..., n$\n",
    "\n",
    "For this model, we know that $\\mu$ is a normal distribution. But suppose we decide that our prior believes about $\\mu$ are better reflected using a standard $t$ distribution with one degree of freedom. We can write that as:\n",
    "\n",
    "$\\mu \\sim \\mathcal{t}(0,1,1)$ This particular prior distribution has heavier tails than the conjugate than the normal distribution, which can more easily accommodate the possibility of extreme values for $\\mu$. \n",
    "\n",
    "The posterior distribution is:\n",
    "\n",
    "$$Pr( \\mu | y_1, ..., y_n) \\propto \\prod_{i=1} [ \\frac{1}{\\sqrt{2 \\pi} } exp(-\\frac{1}{2}(y_i - \\mu)^2)] \\frac{1}{\\pi (1 + \\mu^2)}$$\n",
    "Removing the constants and applying the exponential product rule, we get:\n",
    "$$~~~\\propto exp[ -\\frac{1}{2} \\sum_{i=1} (y_i - \\mu)^2 ] \\frac{1}{1+\\mu^2}$$\n",
    "$$~~~~~~~~~~~~~~~~~~~~~~~~\\propto exp[ -\\frac{1}{2} ( \\sum_{i=1} y_i^2 - 2 \\mu \\sum_{i=1}y_i+ n\\mu^2) ] \\frac{1}{1+\\mu^2}$$\n",
    "$$ \\propto \\frac{ exp[ n ( \\bar{y} \\mu - \\frac{\\mu^2}{2})] }{1 + \\mu^2} ~~~~~~~~~~~~~~~~~~~$$\n",
    "\n",
    "We cannot recognize this expression as being proportional to a common distribution.\n",
    "\n",
    "**EXAMPLE 2**\n",
    "\n",
    "In this example, both $\\mu$ and $\\sigma^2$ are unknown. We will use a normal distribution for $\\mu$ and an inverse gamma distribution for $\\sigma^2$. The model is:\n",
    "\n",
    "$y_i~|~\\mu, \\sigma^2 \\sim ^{iid} N(\\mu, \\sigma^2) ~~~~ i = 1, ..., n$\n",
    "\n",
    "$ \\mu \\sim N( \\mu_o, \\sigma_o^2)$\n",
    "\n",
    "$\\sigma^2 \\sim IG(\\alpha_o, \\beta_o)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's image that we have some function of $\\theta$ from which we would like to compute the following integral:\n",
    "\n",
    "$$\\int_{0}^{\\infty} h(\\theta) Pr(\\theta) d\\theta = E[ h(\\theta) ]$$\n",
    "We can calculate this integral by taking the sample mean evaluated at each of these samples. If we were to calculate the sample mean where we evaluate the h function on each of our simulated samples of $\\theta$. This quantity would approximate this expected value which is this integral:\n",
    "\n",
    "$$\\int_{0}^{\\infty} h(\\theta) Pr(\\theta) d\\theta = E[ h(\\theta) ] \\propto \\frac{1}{m} \\sum_{i=1}^m h(\\theta_i^m)$$\n",
    "\n",
    "One extremely useful example of such h function is the indicator function, $I_A($\\theta$), where A would be some sort of logical condition about the value of $\\theta$. \n",
    "\n",
    "Example.\n",
    "\n",
    "$$h(\\theta) = I_{\\theta < 5 (\\theta)}$$\n",
    "\n",
    "This function will return $1$ if $\\theta < 5$ and $0$ otherwise. The expected value is:\n",
    "\n",
    "\n",
    "$$h(\\theta) = I_{\\theta < 5 (\\theta)} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ $$\n",
    "$$E( h(\\theta) ) = \\int_0^{\\infty} I_{\\theta < 5} (\\theta) ~ Pr(\\theta)~d \\theta$$\n",
    "$$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ = \\int_0^{5} 1 Pr(\\theta) d \\theta + \\int_5^{\\infty} 0 Pr(\\theta) d \\theta $$\n",
    "$$  = Pr(0 < \\theta < 5)$$\n",
    "$$  \\propto \\frac{1}{m} \\sum_{i=1}^{m} I_{\\theta^* < 5} ( \\theta_i^*) $$\n",
    "\n",
    " \n",
    "This means that we could approximate this probability by drawing many samples of $\\theta_{i}^{*}$, and approximating this integral with the sample mean of these indicator functions where $\\theta_i^{*} < 5$ and apply it to our simulated values. It counts how many samples meet the criteria and divides it by the total number of samples. This is a very useful technique to approximate probabilities of events that are difficult to compute analytically.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
